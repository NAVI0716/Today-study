{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ac5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c293b",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da977ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\student\\\\Desktop\\\\ch14_RNN_인코더_디코더\\\\fra-eng.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "zipfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7875573",
   "metadata": {},
   "outputs": [],
   "source": [
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac6b681e",
   "metadata": {},
   "source": [
    "이번 실습에서는 약 19만개의 데이터 중 33,000개의 샘플만을 사용할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e5b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "raw",
   "id": "476d2de9",
   "metadata": {},
   "source": [
    "전처리 함수들을 구현합니다. 구두점 등을 제거하거나 단어와 구분해주기 위한 전처리입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f792c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii(s):\n",
    "    # 프랑스어 악센트(accent) 삭제\n",
    "    # 예시 : 'déjà diné' -> deja dine\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    # 악센트 제거 함수 호출\n",
    "    sent = to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백 추가.\n",
    "    # ex) \"I am a student.\" => \"I am a student .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    # 다수 개의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d22c257",
   "metadata": {},
   "source": [
    "구현한 전처리 함수들을 임의의 문장을 입력으로 테스트해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71a8e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :', preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a592020c",
   "metadata": {},
   "source": [
    "전체 데이터에서 33,000개의 샘플에 대해서 전처리를 수행합니다. \n",
    "또한 훈련 과정에서 교사 강요(Teacher Forcing)을 사용할 예정이므로, 훈련 시 사용할 디코더의 입력 시퀀스와 실제값. 즉, 레이블에 해당되는 출력 시퀀스를 따로 분리하여 저장합니다. \n",
    "입력 시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가하고, \n",
    "출력 시퀀스에는 종료를 의미하는 토큰인 <eos>를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e6cc773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"fra.txt\", \"r\", encoding=\"UTF-8\") as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c90e22a",
   "metadata": {},
   "source": [
    "이렇게 얻은 3개의 데이터셋 인코더의 입력, 디코더의 입력, 디코더의 레이블을 상위 5개 샘플만 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31d9855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "046a7cf9",
   "metadata": {},
   "source": [
    "모델을 설계하기 전 의아한 점이 있을 수 있습니다. 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다고 설명하였는데 디코더의 입력에 해당하는 데이터인 sents_fra_in이 왜 필요할까요?\n",
    "\n",
    "<교사 강요>\n",
    "훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 겁니다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다. 이런 상황이 반복되면 훈련 시간이 느려집니다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. \n",
    "이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7488ad8a",
   "metadata": {},
   "source": [
    "케라스 토크나이저를 통해 단어 집합을 생성, 정수 인코딩을 진행 후 이어서 패딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fbd56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0668956b",
   "metadata": {},
   "source": [
    "데이터의 크기(shape)를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "feece12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 8)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d44eee1",
   "metadata": {},
   "source": [
    "샘플은 총 33,000개 존재하며 \n",
    "영어 문장의 길이는 8, 프랑스어 문장의 길이는 16입니다. \n",
    "단어 집합의 크기를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4da7417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4672, 프랑스어 단어 집합의 크기 : 8153\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2889edc4",
   "metadata": {},
   "source": [
    "단어 집합의 크기는 각각 영어 4,672개와 프랑스어 8,153개입니다. \n",
    "\n",
    "단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 만들어줍니다. \n",
    "이들은 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e970ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d5764a9",
   "metadata": {},
   "source": [
    "테스트 데이터를 분리하기 전 데이터를 섞어줍니다. \n",
    "이를 위해서 순서가 섞인 정수 시퀀스 리스트를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9fd77760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [27165 32454  8185 ... 23919 10481  4250]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :', indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "577ac2dc",
   "metadata": {},
   "source": [
    "이를 데이터셋의 순서로 지정해주면 샘플들이 기존 순서와 다른 순서로 섞이게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea355a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc0683eb",
   "metadata": {},
   "source": [
    "임의로 30,997번째 샘플을 출력해봅시다. \n",
    "이때 decoder_input과 decoder_target은 데이터의 구조상으로 앞에 붙은 <sos> 토큰과 뒤에 붙은 <eos>을 제외하면 동일한 정수 시퀀스를 가져야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba6b4bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14, 1593,    7,  537,    1,    0,    0,    0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77a24b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   22, 1975,    5,  904,    1,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13899050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  22, 1975,    5,  904,    1,    3,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target[30997]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be2d80a1",
   "metadata": {},
   "source": [
    "위의 경우 22, 1975, 5, 904, 1 이라는 동일 시퀀스를 확인했습니다. \n",
    "\n",
    "이제 훈련 데이터의 10%를 테스트 데이터로 분리하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0702e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(33000*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "942d968d",
   "metadata": {},
   "source": [
    "33,000개의 10%에 해당되는 3,300개의 데이터를 테스트 데이터로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28699aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bdf673d",
   "metadata": {},
   "source": [
    "훈련 데이터와 테스트 데이터의 크기(shape)를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e3b5c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 8)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 8)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6e45976",
   "metadata": {},
   "source": [
    "훈련 데이터의 샘플은 29,700개, 테스트 데이터의 샘플은 3,300개가 존재합니다. 이제 모델을 설계합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab7156",
   "metadata": {},
   "source": [
    "# 2. 기계 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd6b4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "20c0dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64로 사용합니다.\n",
    "embedding_dim = 64\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e595699e",
   "metadata": {},
   "source": [
    "인코더를 설계합니다. 인코더를 주목해보면 함수형 API(functional API)를 사용한다는 것 외에는 앞서 다른 실습에서 본 LSTM 설계와 크게 다르지는 않습니다. \n",
    "\n",
    "Masking은 패딩 토큰인 숫자 0의 경우에는 연산을 제외하는 역할을 수행합니다. \n",
    "\n",
    "인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정합니다. 인코더에 입력을 넣으면 내부 상태를 리턴합니다.\n",
    "\n",
    "LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 RNN 챕터에서 LSTM을 처음 설명할 때 언급하였던 은닉 상태와 셀 상태에 해당됩니다. 이 두 가지 상태를 encoder_states에 저장합니다. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달할 예정입니다. 이것이 앞서 배운 컨텍스트 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d4346393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e55d1dab",
   "metadata": {},
   "source": [
    "디코더는 인코더의 마지막 은닉 상태로부터 초기 은닉 상태를 얻습니다. initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당됩니다. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않습니다. \n",
    "\n",
    "seq2seq의 디코더는 기본적으로 각 시점마다 다중 클래스 분류 문제를 풀고있습니다. 매 시점마다 프랑스어 단어 집합의 크기(tar_vocab_size)의 선택지에서 단어를 1개 선택하여 이를 이번 시점에서 예측한 단어로 택합니다. 다중 클래스 분류 문제이므로 출력층으로 소프트맥스 함수와 손실 함수를 크로스 엔트로피 함수를 사용합니다.\n",
    "\n",
    "categorical_crossentropy를 사용하려면 레이블은 원-핫 인코딩이 된 상태여야 합니다. 그런데 현재 decoder_outputs의 경우에는 원-핫 인코딩을 하지 않은 상태입니다. 원-핫 인코딩을 하지 않은 상태로 정수 레이블에 대해서 다중 클래스 분류 문제를 풀고자 하는 경우에는 categorical_crossentropy가 아니라 sparse_categorical_crossentropy를 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2338e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 모델의 입력과 출력을 정의.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "492b86da",
   "metadata": {},
   "source": [
    "모델을 훈련합니다. 128개의 배치 크기로 총 50 에포크 학습합니다. \n",
    "테스트 데이터를 검증 데이터로 사용하여 훈련이 제대로 되고있는지 모니터링하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b36f9a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "233/233 [==============================] - 35s 148ms/step - loss: 1.8556 - acc: 0.6708 - val_loss: 1.7533 - val_acc: 0.7253\n",
      "Epoch 2/10\n",
      "233/233 [==============================] - 33s 141ms/step - loss: 1.6552 - acc: 0.7431 - val_loss: 1.5892 - val_acc: 0.7507\n",
      "Epoch 3/10\n",
      "233/233 [==============================] - 34s 145ms/step - loss: 1.5138 - acc: 0.7579 - val_loss: 1.4785 - val_acc: 0.7629\n",
      "Epoch 4/10\n",
      "233/233 [==============================] - 35s 148ms/step - loss: 1.4190 - acc: 0.7680 - val_loss: 1.4077 - val_acc: 0.7728\n",
      "Epoch 5/10\n",
      "233/233 [==============================] - 32s 139ms/step - loss: 1.3447 - acc: 0.7802 - val_loss: 1.3438 - val_acc: 0.7831\n",
      "Epoch 6/10\n",
      "233/233 [==============================] - 32s 136ms/step - loss: 1.2710 - acc: 0.7920 - val_loss: 1.2782 - val_acc: 0.7948\n",
      "Epoch 7/10\n",
      "233/233 [==============================] - 32s 136ms/step - loss: 1.2055 - acc: 0.8020 - val_loss: 1.2283 - val_acc: 0.8044\n",
      "Epoch 8/10\n",
      "233/233 [==============================] - 31s 132ms/step - loss: 1.1512 - acc: 0.8095 - val_loss: 1.1847 - val_acc: 0.8102\n",
      "Epoch 9/10\n",
      "233/233 [==============================] - 32s 136ms/step - loss: 1.1026 - acc: 0.8163 - val_loss: 1.1465 - val_acc: 0.8164\n",
      "Epoch 10/10\n",
      "233/233 [==============================] - 32s 137ms/step - loss: 1.0565 - acc: 0.8225 - val_loss: 1.1090 - val_acc: 0.8220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a50882a100>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b8a1dc8",
   "metadata": {},
   "source": [
    "저자의 경우 최종 에포크에서 훈련 데이터는 92%의 정확도를, 테스트 데이터에서는 86%의 정확도를 얻었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ce837",
   "metadata": {},
   "source": [
    "# 3. seq2seq 기계 번역기 동작시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19d36fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b6cdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # <SOS>에 해당하는 정수 생성\n",
    "  target_seq = np.zeros((1,1))\n",
    "  target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "  stop_condition = False\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  # stop_condition이 True가 될 때까지 루프 반복\n",
    "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "  while not stop_condition:\n",
    "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # 예측 결과를 단어로 변환\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "    decoded_sentence += ' '+sampled_char\n",
    "\n",
    "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "    if (sampled_char == '<eos>' or\n",
    "        len(decoded_sentence) > 50):\n",
    "        stop_condition = True\n",
    "\n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "849f554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cb04821",
   "metadata": {},
   "source": [
    "훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5815abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 901ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "입력문장 : i m still at home . \n",
      "정답문장 : je suis encore chez moi . \n",
      "번역문장 : je suis un voiture . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "입력문장 : it s too expensive . \n",
      "정답문장 : c est trop cher ! \n",
      "번역문장 : c est un tres . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "입력문장 : tom is a detective . \n",
      "정답문장 : tom est detective . \n",
      "번역문장 : tom est la . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "입력문장 : tom is sobbing . \n",
      "정답문장 : tom sanglote . \n",
      "번역문장 : tom est . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "입력문장 : i lost the game . \n",
      "정답문장 : j ai perdu le match . \n",
      "번역문장 : j ai besoin de la . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6b60ed2",
   "metadata": {},
   "source": [
    "테스트 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6d32b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "입력문장 : i need help now . \n",
      "정답문장 : c est maintenant que j ai besoin d aide . \n",
      "번역문장 : je suis pas . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "입력문장 : i despise you . \n",
      "정답문장 : je vous meprise . \n",
      "번역문장 : je suis . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "입력문장 : i have had it . \n",
      "정답문장 : j en ai ma claque . \n",
      "번역문장 : je suis un voiture . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "입력문장 : i felt a bit tired . \n",
      "정답문장 : je me sentais un peu fatiguee . \n",
      "번역문장 : je suis pas . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "입력문장 : how about tomorrow ? \n",
      "정답문장 : que dis tu de demain ? \n",
      "번역문장 : comment tu as ? \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a33f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddd805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
