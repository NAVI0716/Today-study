{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064e6bac",
   "metadata": {},
   "source": [
    "## 자연어 전처리-영어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384ead8",
   "metadata": {},
   "source": [
    "모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1244f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer #전처리 : 토큰화\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # 시퀸스 패딩이지만 정확한 용도 ????????????????\n",
    "from sklearn.preprocessing import LabelEncoder # ????\n",
    "from sklearn.model_selection import train_test_split # 테스트모델과 트레이닝 모델 생성\n",
    "from tensorflow.keras.utils import to_categorical #y.shape 변형하는데 사용했지만 정확한 용도는 ??????????????? \n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords # 불용어처리\n",
    "from bs4 import BeautifulSoup #html 파서 ??????\n",
    "import re #???????????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6dc805",
   "metadata": {},
   "source": [
    "전처리단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_text, remove_stopwords=False):\n",
    "    X_text = BeautifulSoup(X_text, 'lxml').get_text()#문서에서 단어를 가져옴\n",
    "    X_text = re.sub(\"[^a-zA-Z]\", \" \", X_text) # 영어 소문자에서 대문자 제외하고 전부 공백처리???????\n",
    "    words = X_text.lower().split()#x데이터 소문자화 공백구분\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))#영어 불용어 단어 리스트\n",
    "        #stops.add(불용어 문자열)\n",
    "        words = [w for w in words if not w in stops]#불용어가 없으면 리스트 등록\n",
    "        clean_text = ' '.join(words)#등록된 리스트 목록을 공백기준으로 병합\n",
    "    else:\n",
    "        clean_text = ' '.join(words)\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097aa619",
   "metadata": {},
   "source": [
    "모델불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc68c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('spam.csv',encoding='latin1')[['v1','v2']]#데이터 v1과 v2를 불러옴\n",
    "data=data.rename(columns = {'v1':'y', 'v2' : 'X'}) # 데이터 이름변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b02ef0",
   "metadata": {},
   "source": [
    "불용어처리하기ㅡ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52695bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gurwl\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data['clean_X']=data['X'].apply(lambda x : preprocessing(X_text=x, remove_stopwords=True))\n",
    "#불러온 data의 x값을위의 전처리험수를 사용해 불용어 소문자 영어단어만 블러와준다.\n",
    "#data['X'].apply이부분은 무슨 용도인지 ???????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5993d6fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['y_name']=data['y']\n",
    "data['encoder_y']=LabelEncoder().fit_transform(data['y'])# LabelEncoder()?????????\n",
    "data['categorical_y']=list(to_categorical(data['encoder_y']))#y.shape 변형하는데 사용했지만 정확한 용도는????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9de05a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any() #data파일에 공백 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d196d1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5055, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_X'].nunique(), data['y'].nunique() # 값중 서로다른값이 몇개인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610dda41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gurwl\\AppData\\Local\\Temp\\ipykernel_13220\\3829374632.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['clean_X'] = data['clean_X'].str.replace(\"[^a-zA-Z0-9 ]\",\"\")#알파벳,숫자,공백만\n",
      "C:\\Users\\gurwl\\AppData\\Local\\Temp\\ipykernel_13220\\3829374632.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['clean_X'] = data['clean_X'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n"
     ]
    }
   ],
   "source": [
    "data['clean_X'] = data['clean_X'].str.replace(\"[^a-zA-Z0-9 ]\",\"\")#영대문자,숫자,공백 제외한 문자는 공백처리\n",
    "data['clean_X'] = data['clean_X'].str.replace('^ +', \"\")#문장 첫단 공백제거\n",
    "data['clean_X'].replace('', np.nan, inplace=True)#공백을 nan으로 변경\n",
    "data = data.dropna(how = 'any')#nan값의 줄을 빼낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2360bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=data['encoder_y']#y데이터지정\n",
    "#Y=to_categorical(data['encoder_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da06721",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data['clean_X']#x데이터지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0a1f6",
   "metadata": {},
   "source": [
    "train_test 나눌때 x->x_data로 나눈것들 다시 train,val로 나눠서 쓰는구나는 알겠는데 나누는 규칙이나 방법은 잘모르겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d13a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3413d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data,test_x,y_data,test_y = train_test_split(X,Y,test_size=0.3,random_state=0)#x,y데이터와 테스트데이터 비율 7:3,랜덤값 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aca3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,val_x,train_y,val_y = train_test_split(x_data,y_data,test_size=0.2,random_state=0)#트레이닝데이터와 검증데이터 비율 8:2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c066f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token=Tokenizer()#토큰단어클래스 불러옴\n",
    "token.fit_on_texts(train_x)#train_x 단어들을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b4bfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_train_x=token.texts_to_sequences(train_x)## 시퀸스화라고하는데 정확한 용도??????????? 숫자화??\n",
    "token_test_x=token.texts_to_sequences(test_x)## 시퀸스화라고하는데 정확한 용도???????????\n",
    "token_val_x=token.texts_to_sequences(val_x)## 시퀸스화라고하는데 정확한 용도???????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50327bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = pad_sequences(token_train_x)#패딩작업 큰값에 맞춰 나머지 빈값들 0으로 채워줌\n",
    "test_inputs = pad_sequences(token_test_x)\n",
    "val_inputs = pad_sequences(token_val_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
