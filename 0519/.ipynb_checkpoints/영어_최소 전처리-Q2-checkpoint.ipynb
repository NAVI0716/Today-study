{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064e6bac",
   "metadata": {},
   "source": [
    "## 자연어 전처리-영어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399f647",
   "metadata": {},
   "source": [
    "관련 패키지 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1244f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer#토큰화\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences#패딩\n",
    "from sklearn.preprocessing import LabelEncoder#정답 숫자화\n",
    "from sklearn.model_selection import train_test_split#데이더 분할\n",
    "from tensorflow.keras.utils import to_categorical#정답을 원_핫 인코딩화\n",
    "import numpy as np#넘파이 \n",
    "from nltk.corpus import stopwords#불용어 메소드\n",
    "from bs4 import BeautifulSoup#마크업 단어를 정리하기 위한 목적\n",
    "import re#문자 정규표준\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81af619",
   "metadata": {},
   "source": [
    "전처리 메소드 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada8c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_text, remove_stopwords=False):\n",
    "    X_text = BeautifulSoup(X_text, 'lxml').get_text()#마크업언더 정리\n",
    "    X_text = re.sub(\"[^a-zA-Z]\", \" \", X_text)#영어 숫자 말고 제거\n",
    "    words = X_text.lower().split()#소문자화\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))#불용어 로드\n",
    "        #stops.add(불용어 문자열)#추가 불용어가 필요할 기록<문자열1개>\n",
    "        words = [w for w in words if not w in stops]\n",
    "        clean_text = ' '.join(words)\n",
    "    else:\n",
    "        clean_text = ' '.join(words)\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc68c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('spam.csv',encoding='latin1')[['v1','v2']]\n",
    "data=data.rename(columns = {'v1':'y', 'v2' : 'X'})#데이터 프레임 열이름 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52695bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_X']=data['X'].apply(lambda x : preprocessing(X_text=x, remove_stopwords=True))#만들어진 함수이용 data 1차 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5993d6fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['y_name']=data['y']#정답 이름 기록\n",
    "data['encoder_y']=LabelEncoder().fit_transform(data['y'])#정답 숫자화\n",
    "data['categorical_y']=list(to_categorical(data['encoder_y']))#정답 다중값 희소행렬정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9de05a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()#결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d196d1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5055, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_X'].nunique(), data['y'].nunique()#중복 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610dda41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\류성빈\\AppData\\Local\\Temp/ipykernel_5292/3168660846.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['clean_X'] = data['clean_X'].str.replace(\"[^a-zA-Z0-9 ]\",\"\")#영어 숫자 말고 제거\n",
      "C:\\Users\\류성빈\\AppData\\Local\\Temp/ipykernel_5292/3168660846.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['clean_X'] = data['clean_X'].str.replace('^ +', \"\")#문장 앞의 공백 제거\n"
     ]
    }
   ],
   "source": [
    "data=data.drop_duplicates(subset=['X'])#중복 제거\n",
    "data['clean_X'] = data['clean_X'].str.replace(\"[^a-zA-Z0-9 ]\",\"\")#영어 숫자 말고 제거\n",
    "data['clean_X'] = data['clean_X'].str.replace('^ +', \"\")#문장 앞의 공백 제거\n",
    "data['clean_X'].replace('', np.nan, inplace=True)#비어있는 문자열 NaN화\n",
    "data = data.dropna(how = 'any')#NaN_ data 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2360bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.array(data['encoder_y'])#이진 분류\n",
    "#Y=to_categorical(data['encoder_y'])#다중 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da06721",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(data['clean_X'])#입력 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3413d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data,test_x,y_data,test_y = train_test_split(X,Y,test_size=0.3,stratify=Y,random_state=0)#태스트 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aca3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,val_x,train_y,val_y = train_test_split(x_data,y_data,test_size=0.2,stratify=y_data,random_state=0)#학습데이터,검증데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c066f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(train_x)#입력된 데이터 내의 단어모음집 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1314b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len([d for d in sorted(list(tk.word_counts.items()),key=lambda x:x[1]) if d[1]>4])+1#단어모음집중 4번이하 제거 단어갯수기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c19ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=Tokenizer(n)#정의한 단어의 수를 기반으로 문서 정리\n",
    "token.fit_on_texts(train_x)#학습 데이터를 이용하여 단어모음집을 가진 장치 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb5f53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_train_x=token.texts_to_sequences(train_x)#같은 사전을 이용하여 같은 범주의 단어들을 가지고있는 단어 표현\n",
    "token_test_x=token.texts_to_sequences(test_x)#같은 사전을 이용하여 같은 범주의 단어들을 가지고있는 단어 표현\n",
    "token_val_x=token.texts_to_sequences(val_x)#같은 사전을 이용하여 같은 범주의 단어들을 가지고있는 단어 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da8db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(token_train_x) if len(sentence) < 1]#빈문자열 위치 확인\n",
    "drop_test = [index for index, sentence in enumerate(token_test_x) if len(sentence) < 1]#빈문자열 위치 확인\n",
    "drop_val = [index for index, sentence in enumerate(token_val_x) if len(sentence) < 1]#빈문자열 위치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd34781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "token_train_x = np.delete(token_train_x, drop_train, axis=0)#빈문자열 제거\n",
    "train_y = np.delete(train_y, drop_train, axis=0)#빈문자열 제거\n",
    "token_test_x = np.delete(token_test_x, drop_test, axis=0)#빈문자열 제거\n",
    "test_y = np.delete(test_y, drop_test, axis=0)#빈문자열 제거\n",
    "token_val_x = np.delete(token_val_x, drop_val, axis=0)#빈문자열 제거\n",
    "val_y = np.delete(val_y, drop_val, axis=0)#빈문자열 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6e4667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_l=len(pad_sequences(token_train_x)[0])#학습 데이터중 가장 긴 문장 길이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38631b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = pad_sequences(token_train_x,maxlen=w_l)#모든 문장의 길이가 똑같게 만든다\n",
    "test_inputs = pad_sequences(token_test_x,maxlen=w_l)#모든 문장의 길이가 똑같게 만든다\n",
    "val_inputs = pad_sequences(token_val_x,maxlen=w_l)#모든 문장의 길이가 똑같게 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf1ee0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs=train_y#용어 정리\n",
    "test_outputs=test_y#용어 정리\n",
    "val_outputs=val_y#용어 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "954182e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((2844, 46), (2844,)), ((1513, 46), (1513,)), ((710, 46), (710,)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_inputs.shape,train_outputs.shape),(test_inputs.shape,test_outputs.shape),(val_inputs.shape,val_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c59f7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Embedding, Dense, LSTM,BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model_eg.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "431097da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n, 100))\n",
    "model.add(LSTM(128,dropout=0.5,return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128,dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/48 [==============================] - 9s 118ms/step - loss: 0.2196 - acc: 0.9195 - val_loss: 0.2997 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.87183, saving model to best_model_eg.h5\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 5s 109ms/step - loss: 0.0607 - acc: 0.9817 - val_loss: 0.2983 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.87183\n",
      "Epoch 3/100\n",
      "24/48 [==============>...............] - ETA: 2s - loss: 0.0399 - acc: 0.9889"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(train_inputs, train_outputs, epochs=100, callbacks=[es, mc], batch_size=60,validation_data=(val_inputs,val_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bfaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('m1 loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model_eg.h5')\n",
    "loaded_model.evaluate(test_inputs,test_outputs)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90122acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def 문장_전처리(문장):\n",
    "    문장 = preprocessing(X_text=문장, remove_stopwords=True)\n",
    "    encoded = token.texts_to_sequences([문장])\n",
    "    pad_new = pad_sequences(encoded, maxlen = w_l) \n",
    "    score = float(loaded_model.predict(pad_new))\n",
    "    if(score > 0.5):\n",
    "        print(f\"{score * 100:.2f}% 확률로 스팸입니다.\\n\")\n",
    "    else:\n",
    "        print(f\"{(1 - score) * 100:.2f}% 확률로 정상메일 입니다.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "문장_전처리(\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e1d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "문장_전처리('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
